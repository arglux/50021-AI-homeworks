{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUbEmuvZJxlI"
   },
   "source": [
    "# PyTorch - homework 2: neural networks\n",
    "\n",
    "-- Prof. Dorien Herremans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efS07mO7J6AR"
   },
   "source": [
    "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mJpzFaX0J6Zz",
    "outputId": "33e49695-d65f-4948-f0ea-057c6f3ad787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homework by Calvin Yusnoveri, number: 1002911\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "student_number = \"1002911\"\n",
    "student_name = \"Calvin Yusnoveri\"\n",
    "\n",
    "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xDkwBg8LKQ_"
   },
   "source": [
    " ## Question 1 -- XOR neural network [3pts]\n",
    "\n",
    "a) Train an (at least) 2-layer neural network that can solve the XOR problem. Hint: be sure to check both this week and last week's lab. \n",
    "\n",
    "b) Check the predictions resulting from your model in the second code box below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "BINvhm-PLKak",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "Epoch: 0, Loss: 0.2929750680923462\n",
      "Epoch: 50, Loss: 0.24973316490650177\n",
      "Epoch: 100, Loss: 0.21934162080287933\n",
      "Epoch: 150, Loss: 0.022789159789681435\n",
      "Epoch: 200, Loss: 1.4584457858290989e-05\n",
      "Epoch: 250, Loss: 5.269632765703136e-07\n",
      "Epoch: 300, Loss: 5.461580632193375e-10\n",
      "Epoch: 350, Loss: 2.0189183658203547e-11\n",
      "Epoch: 400, Loss: 1.176836406102666e-13\n",
      "Epoch: 450, Loss: 7.216449660063518e-15\n",
      "Epoch: 500, Loss: 2.7755575615628914e-16\n",
      "Epoch: 550, Loss: 2.7755575615628914e-16\n",
      "Epoch: 600, Loss: 0.0\n",
      "Epoch: 650, Loss: 0.0\n",
      "Epoch: 700, Loss: 0.0\n",
      "Epoch: 750, Loss: 0.0\n",
      "Epoch: 800, Loss: 0.0\n",
      "Epoch: 850, Loss: 0.0\n",
      "Epoch: 900, Loss: 0.0\n",
      "Epoch: 950, Loss: 0.0\n",
      "Epoch: 1000, Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# load your data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "train_set = [ [0, 0], [0, 1], [1, 0], [1, 1] ]\n",
    "train_ans = [ [0, 1, 1, 0] ]\n",
    "x = torch.Tensor(train_set)\n",
    "y = torch.Tensor(train_ans).view(-1,1)\n",
    "\n",
    "print(f\"Test data: {x}\\n {y}\")\n",
    "\n",
    "# name your model xor\n",
    "class xor(nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=1):\n",
    "        super(xor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# define your model loss function, optimizer, etc.\n",
    "device = \"cuda\"\n",
    "model = xor().to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.03)\n",
    "\n",
    "# train the model\n",
    "def train(x, y, model, loss_function, optimizer, device, epochs=1000+1):\n",
    "    for epoch in range(epochs):\n",
    "        features = x.to(device)\n",
    "        target = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(features)\n",
    "        loss = loss_function(prediction, target.view(-1, 1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0: # print every 50 epochs\n",
    "            print (f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "train(x, y, model, loss_function, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./xor-2306-2215.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save(model, fpath):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%d%m-%H%M\")\n",
    "    \n",
    "    save_path = f'{fpath}-{timestamp}'\n",
    "    torch.save(model.state_dict(), save_path) # model is saved in current dir for reproducibility\n",
    "    print(f'Model saved in {save_path}.')\n",
    "    return save_path\n",
    "\n",
    "save_path = save(model, './xor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "51Ra1T6n2r_R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 xor 0 = 0\n",
      "0 xor 1 = 1\n",
      "1 xor 1 = 0\n",
      "1 xor 0 = 1\n"
     ]
    }
   ],
   "source": [
    "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
    "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
    "\n",
    "test = [[0,0],[0,1],[1,1],[1,0]]\n",
    "\n",
    "device = 'cuda'\n",
    "loaded_xor = xor().to(device) # reinitialize model\n",
    "loaded_xor.load_state_dict(torch.load(save_path)) # ./xor-2306-2215\n",
    "model.eval()\n",
    "\n",
    "for trial in test: \n",
    "    Xtest = torch.Tensor(trial).to(device)\n",
    "    y_hat = model(Xtest)\n",
    "    \n",
    "    if y_hat > 0.5:\n",
    "        prediction = 1\n",
    "    else: \n",
    "        prediction = 0\n",
    "\n",
    "    print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqIqD5ZzyUOW"
   },
   "source": [
    "## Question 2  [2pts]\n",
    "\n",
    "Imagine a neural network model for a multilabel classification task. \n",
    "\n",
    "a) Which loss function should you use?\n",
    "\n",
    "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzye9G18PQ0c"
   },
   "source": [
    "```\n",
    "[your answer here, no coding required]\n",
    "\n",
    "* answer A: Multilabel classification tasks are decomposed into N binary classifiers where N is number of labels. \n",
    "So, the loss function is just Binary Cross Entropy loss for each classifier\n",
    "\n",
    "* answer B: High variance error means overfitting.\n",
    "  - 1) Reduce the size of model (i.e. less parameters, less layers) or increase the size of dataset (e.g. data augment)\n",
    "  - 2) Do early stopping\n",
    "  - 3) Add regularization term either L1 or L2 (depending on task) to penalize weights\n",
    "  - 4) Add dropout layer to force the network not to rely on just one node which helps it to generalize better\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcceOSnjjSHf"
   },
   "source": [
    "## Question 3 - Improve hit classification [5pts]\n",
    "\n",
    "Remember the hit predicton dataset from last week? \n",
    "\n",
    "a) Improve the model using a multiplayer perceptron. \n",
    "\n",
    "b) Make sure to run your models on the GPU. \n",
    "\n",
    "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Train data shape: (321, 50), Test data shape: (79, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assuming the data is downloaded and saved in the same dir -> ./content\n",
    "train_data = './content/herremans_hit_1030training.csv'\n",
    "test_data = './content/herremans_hit_1030test.csv'\n",
    "train_data = pd.read_csv(train_data)\n",
    "test_data = pd.read_csv(test_data)\n",
    "print(f\"Data loaded. Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "# print(train_data.head(50))\n",
    "\n",
    "x = torch.FloatTensor(train_data.loc[:, train_data.columns != 'Topclass1030'].values).to(device)\n",
    "y = torch.FloatTensor(train_data['Topclass1030']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "t-jkJDTdjSRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6781057715415955\n",
      "Epoch: 50, Loss: 0.5716888308525085\n",
      "Epoch: 100, Loss: 0.47020143270492554\n",
      "Epoch: 150, Loss: 0.38029322028160095\n",
      "Epoch: 200, Loss: 0.3195233941078186\n",
      "Epoch: 250, Loss: 0.29076775908470154\n",
      "Epoch: 300, Loss: 0.27820831537246704\n",
      "Epoch: 350, Loss: 0.27223384380340576\n",
      "Epoch: 400, Loss: 0.2692500650882721\n",
      "Epoch: 450, Loss: 0.2676442861557007\n",
      "Epoch: 500, Loss: 0.2666953504085541\n",
      "Epoch: 550, Loss: 0.2660396993160248\n",
      "Epoch: 600, Loss: 0.26557835936546326\n",
      "Epoch: 650, Loss: 0.26521551609039307\n",
      "Epoch: 700, Loss: 0.26495152711868286\n",
      "Epoch: 750, Loss: 0.2646833658218384\n",
      "Epoch: 800, Loss: 0.2645995318889618\n",
      "Epoch: 850, Loss: 0.26448795199394226\n",
      "Epoch: 900, Loss: 0.2643808424472809\n",
      "Epoch: 950, Loss: 0.26423314213752747\n",
      "Epoch: 1000, Loss: 0.2642240822315216\n",
      "Epoch: 1050, Loss: 0.2640830874443054\n",
      "Epoch: 1100, Loss: 0.2640083134174347\n",
      "Epoch: 1150, Loss: 0.2639453709125519\n",
      "Epoch: 1200, Loss: 0.26387515664100647\n",
      "Epoch: 1250, Loss: 0.26391148567199707\n",
      "Epoch: 1300, Loss: 0.263852059841156\n",
      "Epoch: 1350, Loss: 0.2638903856277466\n",
      "Epoch: 1400, Loss: 0.26380273699760437\n",
      "Epoch: 1450, Loss: 0.2637636959552765\n",
      "Epoch: 1500, Loss: 0.2637259066104889\n",
      "Epoch: 1550, Loss: 0.2637208104133606\n",
      "Epoch: 1600, Loss: 0.26381218433380127\n",
      "Epoch: 1650, Loss: 0.2637081444263458\n",
      "Epoch: 1700, Loss: 0.2636972963809967\n",
      "Epoch: 1750, Loss: 0.26365476846694946\n",
      "Epoch: 1800, Loss: 0.2637322247028351\n",
      "Epoch: 1850, Loss: 0.26362818479537964\n",
      "Epoch: 1900, Loss: 0.2636169493198395\n",
      "Epoch: 1950, Loss: 0.2635968327522278\n",
      "Epoch: 2000, Loss: 0.2636866569519043\n",
      "Model saved in ./multi-layer-logreg-2306-2210.\n"
     ]
    }
   ],
   "source": [
    "# code your model 1\n",
    "\n",
    "class MultiLayerLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MultiLayerLogisticRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# train model\n",
    "device = 'cuda'\n",
    "num_out = 1\n",
    "num_inp = 49 # 50 total dimensions (num of columns of dataset)\n",
    "\n",
    "lr_rate = 0.001\n",
    "loss_function = nn.BCELoss()\n",
    "model1 = MultiLayerLogisticRegression(num_inp, num_out).to(device)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=lr_rate)\n",
    "\n",
    "train(x, y, model1, loss_function, optimizer, device)\n",
    "save_path = save(model1, './multi-layer-logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UIDPTKcFkETc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 47, True Negatives: 14\n",
      "False Positives: 15, False Negatives: 3\n",
      "Class specific accuracy of correctly predicting a hit song is 0.94\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 1 (called model1 here)\n",
    "\n",
    "device = 'cuda'\n",
    "trained_model1 = MultiLayerLogisticRegression(num_inp, num_out).to(device) # reinitialize model\n",
    "trained_model1.load_state_dict(torch.load(save_path)) # try save_path = ./multi-layer-logreg-2306-2210\n",
    "trained_model1.eval()\n",
    "\n",
    "def run_evaluation(my_model):\n",
    "    test = pd.read_csv('./content/herremans_hit_1030test.csv')\n",
    "    labels = test.iloc[:,-1]\n",
    "    test = test.drop('Topclass1030', axis=1)\n",
    "    testdata = torch.Tensor(test.values)\n",
    "    testlabels = torch.Tensor(labels.values).view(-1,1)\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "\n",
    "    for i in range(0, testdata.size()[0]): \n",
    "        # print(testdata[i].size())\n",
    "        Xtest = torch.Tensor(testdata[i]).to(device)\n",
    "        y_hat = my_model(Xtest)\n",
    "\n",
    "        if y_hat > 0.5:\n",
    "            prediction = 1\n",
    "        else: \n",
    "            prediction = 0\n",
    "\n",
    "        if (prediction == testlabels[i]):\n",
    "            if (prediction == 1):\n",
    "                TP += 1\n",
    "            else: \n",
    "                TN += 1\n",
    "\n",
    "        else:\n",
    "            if (prediction == 1):\n",
    "                FP += 1\n",
    "            else: \n",
    "                FN += 1\n",
    "\n",
    "    print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
    "    print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
    "    rate = TP/(FN+TP)\n",
    "    print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
    "\n",
    "run_evaluation(trained_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xghPDDNmkHn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6838576197624207\n",
      "Epoch: 50, Loss: 0.6308794021606445\n",
      "Epoch: 100, Loss: 0.5846298933029175\n",
      "Epoch: 150, Loss: 0.5645995140075684\n",
      "Epoch: 200, Loss: 0.5282886028289795\n",
      "Epoch: 250, Loss: 0.5055448412895203\n",
      "Epoch: 300, Loss: 0.512498676776886\n",
      "Epoch: 350, Loss: 0.480032354593277\n",
      "Epoch: 400, Loss: 0.4498507082462311\n",
      "Epoch: 450, Loss: 0.45719200372695923\n",
      "Epoch: 500, Loss: 0.4388248026371002\n",
      "Epoch: 550, Loss: 0.4245736002922058\n",
      "Epoch: 600, Loss: 0.4117947816848755\n",
      "Epoch: 650, Loss: 0.41078197956085205\n",
      "Epoch: 700, Loss: 0.3838280737400055\n",
      "Epoch: 750, Loss: 0.3869994878768921\n",
      "Epoch: 800, Loss: 0.3886878788471222\n",
      "Epoch: 850, Loss: 0.3684263527393341\n",
      "Epoch: 900, Loss: 0.3625520169734955\n",
      "Epoch: 950, Loss: 0.3505902588367462\n",
      "Epoch: 1000, Loss: 0.3654760718345642\n",
      "Epoch: 1050, Loss: 0.34178394079208374\n",
      "Epoch: 1100, Loss: 0.3386198878288269\n",
      "Epoch: 1150, Loss: 0.337293416261673\n",
      "Epoch: 1200, Loss: 0.33868104219436646\n",
      "Epoch: 1250, Loss: 0.3454767167568207\n",
      "Epoch: 1300, Loss: 0.3516904413700104\n",
      "Epoch: 1350, Loss: 0.3353036940097809\n",
      "Epoch: 1400, Loss: 0.3312843143939972\n",
      "Epoch: 1450, Loss: 0.3292650580406189\n",
      "Epoch: 1500, Loss: 0.3330460786819458\n",
      "Epoch: 1550, Loss: 0.3148905038833618\n",
      "Epoch: 1600, Loss: 0.3200514614582062\n",
      "Epoch: 1650, Loss: 0.3109586238861084\n",
      "Epoch: 1700, Loss: 0.31652796268463135\n",
      "Epoch: 1750, Loss: 0.3275092542171478\n",
      "Epoch: 1800, Loss: 0.3304308354854584\n",
      "Epoch: 1850, Loss: 0.3214685320854187\n",
      "Epoch: 1900, Loss: 0.32767972350120544\n",
      "Epoch: 1950, Loss: 0.33219805359840393\n",
      "Epoch: 2000, Loss: 0.3167402148246765\n",
      "Model saved in ./drop-out-logreg-2306-2210.\n"
     ]
    }
   ],
   "source": [
    "# code your model 2\n",
    "\n",
    "class DropOutLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DropOutLogisticRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.75)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# train model\n",
    "device = 'cuda'\n",
    "num_out = 1\n",
    "num_inp = 49 # 50 total dimensions (num of columns of dataset)\n",
    "\n",
    "lr_rate = 0.001\n",
    "loss_function = nn.BCELoss()\n",
    "model2 = DropOutLogisticRegression(num_inp, num_out).to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=lr_rate)\n",
    "\n",
    "train(x, y, model2, loss_function, optimizer, device)\n",
    "save_path = save(model2, './drop-out-logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "wAIifiHJkHyW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 48, True Negatives: 8\n",
      "False Positives: 21, False Negatives: 2\n",
      "Class specific accuracy of correctly predicting a hit song is 0.96\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 2 (called model2 here)\n",
    "\n",
    "device = 'cuda'\n",
    "trained_model2 = DropOutLogisticRegression(num_inp, num_out).to(device) # reinitialize model\n",
    "trained_model2.load_state_dict(torch.load(save_path)) # try save_path = ./drop-out-logreg-2306-2210\n",
    "trained_model2.eval()\n",
    "\n",
    "run_evaluation(trained_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPsxbT0KkGs1"
   },
   "source": [
    "Which works better and why do you think this may be (very briefly)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GzjI77HkSwH"
   },
   "source": [
    "The first model `model1` are 3 layer logistic regression classifier. The second model `model2` is exactly the same architecture but with a dropout layer attached after the first fully connected layer. They both use the same optimizer and loss function.\n",
    "\n",
    "The second model `model2` seems to work better when basic the metrics on class specific accuracy only. The dropout seems to force the model to not rely on any single particular node too much when training as a subset of it will be turned off randomly. This helps to prevent overfitting.\n",
    "\n",
    "But, if we were to look at it in terms of how much True Positives and True Negatives are predicted, the first model `model1` seems to predict more True Positives and True Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh5O8qS_khug"
   },
   "source": [
    "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI homework PT2 - neural networks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUbEmuvZJxlI"
   },
   "source": [
    "# PyTorch - homework 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efS07mO7J6AR"
   },
   "source": [
    "Please run the whole notebook with your code and submit the `.ipynb` file that includes your answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mJpzFaX0J6Zz",
    "outputId": "33e49695-d65f-4948-f0ea-057c6f3ad787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mHomework by Calvin Yusnoveri, number: 1002911\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "student_number=\"1002911\"\n",
    "student_name=\"Calvin Yusnoveri\"\n",
    "\n",
    "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xDkwBg8LKQ_"
   },
   "source": [
    " ## Question 1 -- matrix multiplication\n",
    "\n",
    "Implement the following mathematical operation on both the CPU and GPU (use Google Colab or another cloud service if you don't have a GPU in your computer). Print:\n",
    "\n",
    "a) which type of GPU card you have and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BINvhm-PLKak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. A) Device: NVIDIA GeForce GTX 1060 -> cuda:0\n"
     ]
    }
   ],
   "source": [
    "# implement solution here\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(device)\n",
    "print(f\"Q1. A) Device: {device_name} -> {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) show the computation time for both CPU and GPU (using PyTorch). \n",
    "\n",
    "c) How much % fast is the GPU? \n",
    "\n",
    " The operation to implement is the dot product $C = B * A^T$\n",
    "\n",
    " whereby $A$ is a random matrix of size $20,000 \\times 1000$ and $B$ is a random matrix of size $2000 \\times 1000$. In addition to the required information asked above:\n",
    " \n",
    " d) please also print the resulting two $C$ matrices (they should be the same btw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dot(A, B, device):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    B = B.to(device)\n",
    "    A = torch.transpose(A, 0, 1).to(device)\n",
    "    \n",
    "    # only compute the matrix multiplication time\n",
    "    start.record()\n",
    "    C = torch.matmul(B, A)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    time = start.elapsed_time(end)\n",
    "    return C, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 B) & D)\n",
      "GPU compute time: 241.7407684326172 Result: \n",
      " tensor([[254.5484, 243.0836, 244.2404,  ..., 252.3349, 236.7377, 246.6936],\n",
      "        [261.7545, 245.7598, 248.8301,  ..., 259.2577, 247.2546, 254.6792],\n",
      "        [256.4694, 251.5695, 249.8525,  ..., 262.8980, 240.3078, 248.4840],\n",
      "        ...,\n",
      "        [250.9371, 248.6571, 245.4411,  ..., 259.8455, 237.5949, 244.0561],\n",
      "        [259.5616, 250.7322, 246.5741,  ..., 255.1424, 239.9357, 252.3823],\n",
      "        [256.1631, 245.3425, 248.6073,  ..., 260.5112, 242.2224, 248.9274]],\n",
      "       device='cuda:0')\n",
      "CPU compute time: 0.0010239999974146485 Result: \n",
      " tensor([[254.5484, 243.0835, 244.2405,  ..., 252.3351, 236.7378, 246.6934],\n",
      "        [261.7546, 245.7597, 248.8299,  ..., 259.2576, 247.2546, 254.6791],\n",
      "        [256.4693, 251.5696, 249.8525,  ..., 262.8979, 240.3079, 248.4841],\n",
      "        ...,\n",
      "        [250.9372, 248.6573, 245.4409,  ..., 259.8455, 237.5949, 244.0561],\n",
      "        [259.5616, 250.7323, 246.5742,  ..., 255.1427, 239.9358, 252.3823],\n",
      "        [256.1630, 245.3425, 248.6072,  ..., 260.5111, 242.2223, 248.9272]])\n",
      "Q1 C) 23607396.97685097 %\n",
      "It seems the computation (although large), is not complex enough to benefit much from my GPU.\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand((20000, 1000))\n",
    "B = torch.rand((2000, 1000))\n",
    "\n",
    "print('Q1 B) & D)')\n",
    "device = 'cuda'\n",
    "C_gpu, gpu_time = calculate_dot(A, B, device)\n",
    "print(f'GPU compute time: {gpu_time} Result: \\n {C_gpu}' )\n",
    "\n",
    "device = 'cpu'\n",
    "C_cpu, cpu_time = calculate_dot(A, B, device)\n",
    "print(f'CPU compute time: {cpu_time} Result: \\n {C_cpu}')\n",
    "\n",
    "print(f'Q1 C) { ((gpu_time / cpu_time) -1) * 100} %') # this is because cpu_time > gpu_time! flip the ratio otherwise!\n",
    "print('It seems the computation (although large), is not complex enough to benefit much from my GPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZJXmfT-yU3g"
   },
   "source": [
    "## Question 2 - grad\n",
    "\n",
    "\n",
    "Find the gradient (partial derivatives) of the function $g(w)$ below. \n",
    "\n",
    "Let  $w=[w_1,w_2]^T$\n",
    "\n",
    "Consider  $g(w)=2w_1w_2+w_2cos(w_1)$\n",
    "\n",
    "a) In PyTorch, compute:   $\\nabla g(w)$ \n",
    "\n",
    " and verify that $\\nabla g([\\pi,1])=[2,2\\piâˆ’1]^T$ using the grad function, whereby the first position is the partial for $w_1$ and the second position is the partial for $w_2$. \n",
    "\n",
    "b) You can also write a function to manually calculate these partial derivatives! You can review your differential equations math at [here](https://www.wolframalpha.com/input/?i=derivative+y+cos%28x%29) and implement this is a second function below to verify that it comes to the same solution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pLjz6_LKt4sc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 A) Autograd: tensor([[2.0000],\n",
      "        [5.2832]])\n",
      "Q2 B) Manual: tensor([[2.0000],\n",
      "        [5.2832]])\n",
      "Answers are the same? tensor([[True],\n",
      "        [True]]) -> should be [[2.0000], [5.2832]]\n"
     ]
    }
   ],
   "source": [
    "# write your solution here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "W = [[np.pi], [1]]\n",
    "w = torch.tensor(W, requires_grad=True)\n",
    "g = 2 * w[0] * w[1]  + w[1] * torch.cos(w[0])\n",
    "dg = torch.autograd.grad(g, w)[0]\n",
    "print(f'Q2 A) Autograd: {dg}')\n",
    "\n",
    "dw1 = 2 * w[1] - torch.sin(w[0]) * w[1]\n",
    "dw2 = 2 * w[0] + torch.cos(w[0])\n",
    "\n",
    "dg_manual = torch.Tensor([[dw1], [dw2]])\n",
    "print(f'Q2 B) Manual: {dg_manual}')\n",
    "\n",
    "print(f'Answers are the same? {torch.eq(dg, dg_manual)} -> should be [[2.0000], [5.2832]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJwP6ur8LKjD"
   },
   "source": [
    "## Question 3 - dance hit song prediction\n",
    "\n",
    "Implement logistic regression in PyTorch for the following dance hit song prediction training dataset: \n",
    "https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
    "\n",
    " * Input variables: a number of audio features (most already standardized so don't worry about that)\n",
    " * Target variable: Topclass1030: \n",
    "   * 1 means it was a top 10 hit song; \n",
    "   * 0 means it never went above top 30 position.\n",
    "\n",
    "This dataset is derived from my paper on dance hit song prediction, for full description of features have a look at https://arxiv.org/abs/1905.08076. \n",
    "\n",
    "Print the evolution of the loss every few epochs and train the model until it converges. \n",
    " \n",
    " After training the logistic regression model, calculate the prediction accuracy on the test set: \n",
    " https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VyRP6bl8t4Wc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Train data shape: (321, 50), Test data shape: (79, 50)\n",
      "Epoch: 0, Loss: 0.7352050542831421\n",
      "Epoch: 50, Loss: 0.7049029469490051\n",
      "Epoch: 100, Loss: 0.6835950613021851\n",
      "Epoch: 150, Loss: 0.6684138774871826\n",
      "Epoch: 200, Loss: 0.6573933959007263\n",
      "Epoch: 250, Loss: 0.649202823638916\n",
      "Epoch: 300, Loss: 0.6429457664489746\n",
      "Epoch: 350, Loss: 0.6380181312561035\n",
      "Epoch: 400, Loss: 0.6340122818946838\n",
      "Epoch: 450, Loss: 0.630652129650116\n",
      "Epoch: 500, Loss: 0.6277498602867126\n",
      "Epoch: 550, Loss: 0.6251769065856934\n",
      "Epoch: 600, Loss: 0.6228451728820801\n",
      "Epoch: 650, Loss: 0.620693564414978\n",
      "Epoch: 700, Loss: 0.6186798214912415\n",
      "Epoch: 750, Loss: 0.6167743802070618\n",
      "Epoch: 800, Loss: 0.6149562001228333\n",
      "Epoch: 850, Loss: 0.6132106184959412\n",
      "Epoch: 900, Loss: 0.6115269064903259\n",
      "Epoch: 950, Loss: 0.609897255897522\n",
      "Epoch: 1000, Loss: 0.6083159446716309\n",
      "Epoch: 1050, Loss: 0.606778621673584\n",
      "Epoch: 1100, Loss: 0.6052818894386292\n",
      "Epoch: 1150, Loss: 0.6038232445716858\n",
      "Epoch: 1200, Loss: 0.6024004220962524\n",
      "Epoch: 1250, Loss: 0.6010116338729858\n",
      "Epoch: 1300, Loss: 0.5996553897857666\n",
      "Epoch: 1350, Loss: 0.5983302593231201\n",
      "Epoch: 1400, Loss: 0.5970351099967957\n",
      "Epoch: 1450, Loss: 0.595768928527832\n",
      "Epoch: 1500, Loss: 0.5945306420326233\n",
      "Epoch: 1550, Loss: 0.5933193564414978\n",
      "Epoch: 1600, Loss: 0.5921341776847839\n",
      "Epoch: 1650, Loss: 0.5909743309020996\n",
      "Epoch: 1700, Loss: 0.5898389220237732\n",
      "Epoch: 1750, Loss: 0.5887273550033569\n",
      "Epoch: 1800, Loss: 0.5876388549804688\n",
      "Epoch: 1850, Loss: 0.5865726470947266\n",
      "Epoch: 1900, Loss: 0.5855281949043274\n",
      "Epoch: 1950, Loss: 0.5845048427581787\n",
      "Epoch: 2000, Loss: 0.5835018754005432\n",
      "Epoch: 2050, Loss: 0.5825189352035522\n",
      "Epoch: 2100, Loss: 0.5815553069114685\n",
      "Epoch: 2150, Loss: 0.5806103944778442\n",
      "Epoch: 2200, Loss: 0.579683780670166\n",
      "Epoch: 2250, Loss: 0.5787749886512756\n",
      "Epoch: 2300, Loss: 0.5778834223747253\n",
      "Epoch: 2350, Loss: 0.5770086646080017\n",
      "Epoch: 2400, Loss: 0.5761502981185913\n",
      "Epoch: 2450, Loss: 0.5753077864646912\n",
      "Epoch: 2500, Loss: 0.5744808316230774\n",
      "Epoch: 2550, Loss: 0.573668897151947\n",
      "Epoch: 2600, Loss: 0.5728716850280762\n",
      "Epoch: 2650, Loss: 0.5720887780189514\n",
      "Epoch: 2700, Loss: 0.5713197588920593\n",
      "Epoch: 2750, Loss: 0.5705643892288208\n",
      "Epoch: 2800, Loss: 0.5698221325874329\n",
      "Epoch: 2850, Loss: 0.569092869758606\n",
      "Epoch: 2900, Loss: 0.5683761835098267\n",
      "Epoch: 2950, Loss: 0.5676717162132263\n",
      "Epoch: 3000, Loss: 0.5669792890548706\n",
      "Epoch: 3050, Loss: 0.5662984251976013\n",
      "Epoch: 3100, Loss: 0.5656290054321289\n",
      "Epoch: 3150, Loss: 0.5649706125259399\n",
      "Epoch: 3200, Loss: 0.5643231272697449\n",
      "Epoch: 3250, Loss: 0.563686192035675\n",
      "Epoch: 3300, Loss: 0.5630596280097961\n",
      "Epoch: 3350, Loss: 0.5624430775642395\n",
      "Epoch: 3400, Loss: 0.5618364214897156\n",
      "Epoch: 3450, Loss: 0.5612394213676453\n",
      "Epoch: 3500, Loss: 0.5606517195701599\n",
      "Epoch: 3550, Loss: 0.5600733160972595\n",
      "Epoch: 3600, Loss: 0.5595038533210754\n",
      "Epoch: 3650, Loss: 0.5589430928230286\n",
      "Epoch: 3700, Loss: 0.5583909749984741\n",
      "Epoch: 3750, Loss: 0.5578472018241882\n",
      "Epoch: 3800, Loss: 0.5573116540908813\n",
      "Epoch: 3850, Loss: 0.5567841529846191\n",
      "Epoch: 3900, Loss: 0.5562644004821777\n",
      "Epoch: 3950, Loss: 0.5557523965835571\n",
      "Epoch: 4000, Loss: 0.5552479028701782\n",
      "Epoch: 4050, Loss: 0.5547508001327515\n",
      "Epoch: 4100, Loss: 0.554260790348053\n",
      "Epoch: 4150, Loss: 0.5537779331207275\n",
      "Epoch: 4200, Loss: 0.5533019304275513\n",
      "Epoch: 4250, Loss: 0.5528326034545898\n",
      "Epoch: 4300, Loss: 0.552370011806488\n",
      "Epoch: 4350, Loss: 0.5519137978553772\n",
      "Epoch: 4400, Loss: 0.5514639616012573\n",
      "Epoch: 4450, Loss: 0.5510203242301941\n",
      "Epoch: 4500, Loss: 0.5505828261375427\n",
      "Epoch: 4550, Loss: 0.5501512289047241\n",
      "Epoch: 4600, Loss: 0.5497255921363831\n",
      "Epoch: 4650, Loss: 0.5493055582046509\n",
      "Epoch: 4700, Loss: 0.5488911271095276\n",
      "Epoch: 4750, Loss: 0.5484822988510132\n",
      "Epoch: 4800, Loss: 0.5480788350105286\n",
      "Epoch: 4850, Loss: 0.547680675983429\n",
      "Epoch: 4900, Loss: 0.5472877025604248\n",
      "Epoch: 4950, Loss: 0.5468998551368713\n",
      "Epoch: 5000, Loss: 0.5465169548988342\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# load data\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "# assuming the data is downloaded and saved in the same dir -> ./content\n",
    "train_data = './content/herremans_hit_1030training.csv'\n",
    "test_data = './content/herremans_hit_1030test.csv'\n",
    "train_data = pd.read_csv(train_data)\n",
    "test_data = pd.read_csv(test_data)\n",
    "print(f\"Data loaded. Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "# print(train_data.head(50))\n",
    "\n",
    "# define logistic regression model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# train model\n",
    "device = 'cuda'\n",
    "epochs = 5000 + 1\n",
    "\n",
    "num_out = 1\n",
    "num_inp = 49 # 50 total dimensions (num of columns of dataset)\n",
    "\n",
    "lr_rate = 0.001\n",
    "loss_function = nn.BCELoss()\n",
    "logreg_clf = LogisticRegression(num_inp, num_out).to(device)\n",
    "optimizer = torch.optim.SGD(logreg_clf.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    features = torch.FloatTensor(train_data.loc[:, train_data.columns != 'Topclass1030'].values).to(device)\n",
    "    target = torch.FloatTensor(train_data['Topclass1030']).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     print(prediction.size())\n",
    "#     print(target.view(-1,1).size())\n",
    "    \n",
    "    prediction = logreg_clf(features)\n",
    "    loss = loss_function(prediction, target.view(-1,1)).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0: # print every 50 epochs\n",
    "        print (f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./logreg-clf-1706-1340.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d%m-%H%M\")\n",
    "save_path = f'./logreg-clf-{timestamp}'\n",
    "torch.save(logreg_clf.state_dict(), save_path) # model is saved in current dir for reproducibility\n",
    "print(f'Model saved in {save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw4yfGoGuChe"
   },
   "source": [
    "Run the below code to test the accuracy of your model on the training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L88WmKtMt5gH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 45, True Negatives: 13\n",
      "False Positives: 16, False Negatives: 5\n",
      "Class specific accuracy of correctly predicting a hit song is 0.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "test = pd.read_csv('./content/herremans_hit_1030test.csv')\n",
    "labels = test.iloc[:,-1]\n",
    "test = test.drop('Topclass1030', axis=1)\n",
    "testdata = torch.Tensor(test.values) \n",
    "testlabels = torch.Tensor(labels.values).view(-1,1)\n",
    "\n",
    "device = 'cuda'\n",
    "trained_logreg_clf = LogisticRegression(num_inp, num_out).to(device) # reinitialize model\n",
    "trained_logreg_clf.load_state_dict(torch.load(save_path)) # load trained model\n",
    "logreg_clf.eval()\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "\n",
    "for i in range(0, testdata.size()[0]):\n",
    "  Xtest = torch.Tensor(testdata[i]).to(device)\n",
    "  y_hat = logreg_clf(Xtest)\n",
    "  \n",
    "  if y_hat > 0.5:\n",
    "    prediction = 1\n",
    "  else: \n",
    "    prediction = 0\n",
    "\n",
    "  if (prediction == testlabels[i]):\n",
    "    if (prediction == 1):\n",
    "      TP += 1\n",
    "    else: \n",
    "      TN += 1\n",
    "\n",
    "  else:\n",
    "    if (prediction == 1):\n",
    "      FP += 1\n",
    "    else: \n",
    "      FN += 1\n",
    "\n",
    "print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
    "print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
    "rate = TP/(FN+TP)\n",
    "print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI homework PT1 - torch intro",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
